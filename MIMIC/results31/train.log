Fri Dec  6 12:17:56 EST 2019
guppy7
/h/laleh/PycharmProjects/Fairness/Nov28/MIMIC/2
  0%|          | 0/65 [00:00<?, ?it/s]Validation_df size: 37300
Train_df size 298137
Test_df size 36421
Validation_df path 37300
Train_df path 298137
Epoch 0/64
----------
0
24000
48000
72000
96000
120000
144000
168000
192000
216000
240000
264000
288000
Train_losses: [0.24837185442447662]
  2%|▏         | 1/65 [3:01:36<193:42:25, 10896.03s/it]0
24000
Validation_losses: [0.24508623778820038]
saving
Epoch 1/64
----------
0
24000
48000
72000
96000
120000
144000
168000
192000
216000
240000
264000
288000
Train_losses: [0.24837185442447662, 0.23836220800876617]
  3%|▎         | 2/65 [5:43:14<184:23:40, 10536.83s/it]0
24000
Validation_losses: [0.24508623778820038, 0.23523275554180145]
saving
Epoch 2/64
----------
0
24000
48000
72000
96000
120000
144000
168000
192000
216000
240000
264000
288000
Train_losses: [0.24837185442447662, 0.23836220800876617, 0.23395410180091858]
  5%|▍         | 3/65 [8:11:45<173:04:07, 10049.15s/it]0
24000
Validation_losses: [0.24508623778820038, 0.23523275554180145, 0.2308899313211441]
saving
Epoch 3/64
----------
0
24000
48000
72000
96000
120000
144000
168000
192000
216000
240000
264000
288000
Train_losses: [0.24837185442447662, 0.23836220800876617, 0.23395410180091858, 0.2309217005968094]
  6%|▌         | 4/65 [10:36:18<163:16:44, 9636.13s/it]0
24000
Validation_losses: [0.24508623778820038, 0.23523275554180145, 0.2308899313211441, 0.24815605580806732]
Epoch 4/64
----------
0
24000
48000
72000
96000
120000
144000
168000
192000
216000
240000
264000
288000
Train_losses: [0.24837185442447662, 0.23836220800876617, 0.23395410180091858, 0.2309217005968094, 0.22854508459568024]
  8%|▊         | 5/65 [12:56:14<154:23:58, 9263.98s/it]0
24000
Validation_losses: [0.24508623778820038, 0.23523275554180145, 0.2308899313211441, 0.24815605580806732, 0.2290923297405243]
saving
Epoch 5/64
----------
0
24000
48000
72000
96000
120000
144000
168000
192000
216000
240000
264000
288000
Train_losses: [0.24837185442447662, 0.23836220800876617, 0.23395410180091858, 0.2309217005968094, 0.22854508459568024, 0.22665882110595703]
  9%|▉         | 6/65 [15:17:15<147:52:47, 9023.19s/it]0
24000
Validation_losses: [0.24508623778820038, 0.23523275554180145, 0.2308899313211441, 0.24815605580806732, 0.2290923297405243, 0.2281692773103714]
saving
Epoch 6/64
----------
0
24000
48000
72000
96000
120000
144000
168000
192000
216000
240000
264000
288000
Train_losses: [0.24837185442447662, 0.23836220800876617, 0.23395410180091858, 0.2309217005968094, 0.22854508459568024, 0.22665882110595703, 0.22489234805107117]
 11%|█         | 7/65 [17:38:06<142:36:29, 8851.53s/it]0
24000
Validation_losses: [0.24508623778820038, 0.23523275554180145, 0.2308899313211441, 0.24815605580806732, 0.2290923297405243, 0.2281692773103714, 0.23243391513824463]
Epoch 7/64
----------
0
24000
48000
72000
96000
120000
144000
168000
192000
216000
240000
264000
288000
Train_losses: [0.24837185442447662, 0.23836220800876617, 0.23395410180091858, 0.2309217005968094, 0.22854508459568024, 0.22665882110595703, 0.22489234805107117, 0.22335195541381836]
 12%|█▏        | 8/65 [19:57:50<137:55:53, 8711.46s/it]0
24000
Validation_losses: [0.24508623778820038, 0.23523275554180145, 0.2308899313211441, 0.24815605580806732, 0.2290923297405243, 0.2281692773103714, 0.23243391513824463, 0.2334224283695221]
Epoch 8/64
----------
0
24000
48000
72000
96000
120000
144000
168000
192000
216000
240000
264000
288000
Train_losses: [0.24837185442447662, 0.23836220800876617, 0.23395410180091858, 0.2309217005968094, 0.22854508459568024, 0.22665882110595703, 0.22489234805107117, 0.22335195541381836, 0.22191108763217926]
 14%|█▍        | 9/65 [22:01:42<129:32:24, 8327.59s/it]0
24000
Validation_losses: [0.24508623778820038, 0.23523275554180145, 0.2308899313211441, 0.24815605580806732, 0.2290923297405243, 0.2281692773103714, 0.23243391513824463, 0.2334224283695221, 0.23414932191371918]
decay loss from 0.0005 to 0.00025 as not seeing improvement in val loss
created new optimizer with LR 0.00025
Epoch 9/64
----------
0
24000
48000
72000
96000
120000
144000
168000
192000
216000
240000
264000
288000
Train_losses: [0.24837185442447662, 0.23836220800876617, 0.23395410180091858, 0.2309217005968094, 0.22854508459568024, 0.22665882110595703, 0.22489234805107117, 0.22335195541381836, 0.22191108763217926, 0.2163100242614746]
 15%|█▌        | 10/65 [24:05:17<123:02:32, 8053.69s/it]0
24000
Validation_losses: [0.24508623778820038, 0.23523275554180145, 0.2308899313211441, 0.24815605580806732, 0.2290923297405243, 0.2281692773103714, 0.23243391513824463, 0.2334224283695221, 0.23414932191371918, 0.24953418970108032]
decay loss from 0.00025 to 0.000125 as not seeing improvement in val loss
created new optimizer with LR 0.000125
Epoch 10/64
----------
0
24000
48000
72000
96000
120000
144000
168000
192000
216000
240000
264000
288000
Train_losses: [0.24837185442447662, 0.23836220800876617, 0.23395410180091858, 0.2309217005968094, 0.22854508459568024, 0.22665882110595703, 0.22489234805107117, 0.22335195541381836, 0.22191108763217926, 0.2163100242614746, 0.21173980832099915]
 17%|█▋        | 11/65 [26:08:02<117:42:26, 7847.16s/it]0
24000
Validation_losses: [0.24508623778820038, 0.23523275554180145, 0.2308899313211441, 0.24815605580806732, 0.2290923297405243, 0.2281692773103714, 0.23243391513824463, 0.2334224283695221, 0.23414932191371918, 0.24953418970108032, 0.22492143511772156]
saving
Epoch 11/64
----------
0
24000
48000
72000
96000
120000
144000
168000
192000
216000
240000
264000
288000
Train_losses: [0.24837185442447662, 0.23836220800876617, 0.23395410180091858, 0.2309217005968094, 0.22854508459568024, 0.22665882110595703, 0.22489234805107117, 0.22335195541381836, 0.22191108763217926, 0.2163100242614746, 0.21173980832099915, 0.20977532863616943]
 18%|█▊        | 12/65 [28:11:18<113:32:03, 7711.77s/it]0
24000
Validation_losses: [0.24508623778820038, 0.23523275554180145, 0.2308899313211441, 0.24815605580806732, 0.2290923297405243, 0.2281692773103714, 0.23243391513824463, 0.2334224283695221, 0.23414932191371918, 0.24953418970108032, 0.22492143511772156, 0.22605453431606293]
Epoch 12/64
----------
0
24000
48000
72000
96000
120000
144000
168000
192000
216000
240000
264000
288000
Train_losses: [0.24837185442447662, 0.23836220800876617, 0.23395410180091858, 0.2309217005968094, 0.22854508459568024, 0.22665882110595703, 0.22489234805107117, 0.22335195541381836, 0.22191108763217926, 0.2163100242614746, 0.21173980832099915, 0.20977532863616943, 0.2082945704460144]
 20%|██        | 13/65 [30:17:13<110:42:52, 7664.86s/it]0
24000
Validation_losses: [0.24508623778820038, 0.23523275554180145, 0.2308899313211441, 0.24815605580806732, 0.2290923297405243, 0.2281692773103714, 0.23243391513824463, 0.2334224283695221, 0.23414932191371918, 0.24953418970108032, 0.22492143511772156, 0.22605453431606293, 0.2261160910129547]
Epoch 13/64
----------
0
24000
48000
72000
96000
120000
144000
168000
192000
216000
240000
264000
288000
Train_losses: [0.24837185442447662, 0.23836220800876617, 0.23395410180091858, 0.2309217005968094, 0.22854508459568024, 0.22665882110595703, 0.22489234805107117, 0.22335195541381836, 0.22191108763217926, 0.2163100242614746, 0.21173980832099915, 0.20977532863616943, 0.2082945704460144, 0.20679637789726257]
 22%|██▏       | 14/65 [32:27:57<109:20:48, 7718.60s/it]0
24000
Validation_losses: [0.24508623778820038, 0.23523275554180145, 0.2308899313211441, 0.24815605580806732, 0.2290923297405243, 0.2281692773103714, 0.23243391513824463, 0.2334224283695221, 0.23414932191371918, 0.24953418970108032, 0.22492143511772156, 0.22605453431606293, 0.2261160910129547, 0.22700588405132294]
decay loss from 0.000125 to 6.25e-05 as not seeing improvement in val loss
created new optimizer with LR 6.25e-05
Epoch 14/64
----------
0
24000
48000
72000
96000
120000
144000
168000
192000
216000
240000
264000
288000
Train_losses: [0.24837185442447662, 0.23836220800876617, 0.23395410180091858, 0.2309217005968094, 0.22854508459568024, 0.22665882110595703, 0.22489234805107117, 0.22335195541381836, 0.22191108763217926, 0.2163100242614746, 0.21173980832099915, 0.20977532863616943, 0.2082945704460144, 0.20679637789726257, 0.2034735232591629]
 23%|██▎       | 15/65 [34:41:05<108:19:19, 7799.18s/it]0
24000
Validation_losses: [0.24508623778820038, 0.23523275554180145, 0.2308899313211441, 0.24815605580806732, 0.2290923297405243, 0.2281692773103714, 0.23243391513824463, 0.2334224283695221, 0.23414932191371918, 0.24953418970108032, 0.22492143511772156, 0.22605453431606293, 0.2261160910129547, 0.22700588405132294, 0.2279358208179474]
decay loss from 6.25e-05 to 3.125e-05 as not seeing improvement in val loss
created new optimizer with LR 3.125e-05
Epoch 15/64
----------
0
24000
48000
72000
96000
120000
144000
168000
192000
216000
240000
264000
288000
Train_losses: [0.24837185442447662, 0.23836220800876617, 0.23395410180091858, 0.2309217005968094, 0.22854508459568024, 0.22665882110595703, 0.22489234805107117, 0.22335195541381836, 0.22191108763217926, 0.2163100242614746, 0.21173980832099915, 0.20977532863616943, 0.2082945704460144, 0.20679637789726257, 0.2034735232591629, 0.201302170753479]
 25%|██▍       | 16/65 [36:53:02<106:38:23, 7834.76s/it]0
24000
Validation_losses: [0.24508623778820038, 0.23523275554180145, 0.2308899313211441, 0.24815605580806732, 0.2290923297405243, 0.2281692773103714, 0.23243391513824463, 0.2334224283695221, 0.23414932191371918, 0.24953418970108032, 0.22492143511772156, 0.22605453431606293, 0.2261160910129547, 0.22700588405132294, 0.2279358208179474, 0.2282657027244568]
decay loss from 3.125e-05 to 1.5625e-05 as not seeing improvement in val loss
created new optimizer with LR 1.5625e-05
Epoch 16/64
----------
0
24000
48000
72000
96000
120000
144000
168000
192000
216000
240000
264000
288000
Train_losses: [0.24837185442447662, 0.23836220800876617, 0.23395410180091858, 0.2309217005968094, 0.22854508459568024, 0.22665882110595703, 0.22489234805107117, 0.22335195541381836, 0.22191108763217926, 0.2163100242614746, 0.21173980832099915, 0.20977532863616943, 0.2082945704460144, 0.20679637789726257, 0.2034735232591629, 0.201302170753479, 0.20006850361824036]
 26%|██▌       | 17/65 [39:05:11<104:50:17, 7862.87s/it]0
24000
Validation_losses: [0.24508623778820038, 0.23523275554180145, 0.2308899313211441, 0.24815605580806732, 0.2290923297405243, 0.2281692773103714, 0.23243391513824463, 0.2334224283695221, 0.23414932191371918, 0.24953418970108032, 0.22492143511772156, 0.22605453431606293, 0.2261160910129547, 0.22700588405132294, 0.2279358208179474, 0.2282657027244568, 0.229500412940979]
decay loss from 1.5625e-05 to 7.8125e-06 as not seeing improvement in val loss
created new optimizer with LR 7.8125e-06
Epoch 17/64
----------
0
24000
48000
72000
96000
120000
144000
168000
192000
216000
240000
264000
288000
Train_losses: [0.24837185442447662, 0.23836220800876617, 0.23395410180091858, 0.2309217005968094, 0.22854508459568024, 0.22665882110595703, 0.22489234805107117, 0.22335195541381836, 0.22191108763217926, 0.2163100242614746, 0.21173980832099915, 0.20977532863616943, 0.2082945704460144, 0.20679637789726257, 0.2034735232591629, 0.201302170753479, 0.20006850361824036, 0.19934922456741333]
 28%|██▊       | 18/65 [41:17:36<102:58:40, 7887.66s/it]0
24000
Validation_losses: [0.24508623778820038, 0.23523275554180145, 0.2308899313211441, 0.24815605580806732, 0.2290923297405243, 0.2281692773103714, 0.23243391513824463, 0.2334224283695221, 0.23414932191371918, 0.24953418970108032, 0.22492143511772156, 0.22605453431606293, 0.2261160910129547, 0.22700588405132294, 0.2279358208179474, 0.2282657027244568, 0.229500412940979, 0.22972556948661804]
decay loss from 7.8125e-06 to 3.90625e-06 as not seeing improvement in val loss
created new optimizer with LR 3.90625e-06
Epoch 18/64
----------
0
24000
48000
72000
96000
120000
144000
168000
192000
216000
240000
264000
288000
Train_losses: [0.24837185442447662, 0.23836220800876617, 0.23395410180091858, 0.2309217005968094, 0.22854508459568024, 0.22665882110595703, 0.22489234805107117, 0.22335195541381836, 0.22191108763217926, 0.2163100242614746, 0.21173980832099915, 0.20977532863616943, 0.2082945704460144, 0.20679637789726257, 0.2034735232591629, 0.201302170753479, 0.20006850361824036, 0.19934922456741333, 0.19912199676036835]
 29%|██▉       | 19/65 [43:30:10<101:02:20, 7907.41s/it]0
24000
Validation_losses: [0.24508623778820038, 0.23523275554180145, 0.2308899313211441, 0.24815605580806732, 0.2290923297405243, 0.2281692773103714, 0.23243391513824463, 0.2334224283695221, 0.23414932191371918, 0.24953418970108032, 0.22492143511772156, 0.22605453431606293, 0.2261160910129547, 0.22700588405132294, 0.2279358208179474, 0.2282657027244568, 0.229500412940979, 0.22972556948661804, 0.22984717786312103]
decay loss from 3.90625e-06 to 1.953125e-06 as not seeing improvement in val loss
created new optimizer with LR 1.953125e-06
Epoch 19/64
----------
0
24000
48000
72000
96000
120000
144000
168000
192000
216000
240000
264000
288000
Train_losses: [0.24837185442447662, 0.23836220800876617, 0.23395410180091858, 0.2309217005968094, 0.22854508459568024, 0.22665882110595703, 0.22489234805107117, 0.22335195541381836, 0.22191108763217926, 0.2163100242614746, 0.21173980832099915, 0.20977532863616943, 0.2082945704460144, 0.20679637789726257, 0.2034735232591629, 0.201302170753479, 0.20006850361824036, 0.19934922456741333, 0.19912199676036835, 0.1988079845905304]
 31%|███       | 20/65 [45:43:04<99:05:27, 7927.28s/it] 0
24000
Validation_losses: [0.24508623778820038, 0.23523275554180145, 0.2308899313211441, 0.24815605580806732, 0.2290923297405243, 0.2281692773103714, 0.23243391513824463, 0.2334224283695221, 0.23414932191371918, 0.24953418970108032, 0.22492143511772156, 0.22605453431606293, 0.2261160910129547, 0.22700588405132294, 0.2279358208179474, 0.2282657027244568, 0.229500412940979, 0.22972556948661804, 0.22984717786312103, 0.22992195188999176]
decay loss from 1.953125e-06 to 9.765625e-07 as not seeing improvement in val loss
created new optimizer with LR 9.765625e-07
Epoch 20/64
----------
0
24000
48000
72000
96000
120000
144000
168000
192000
216000
240000
264000
288000
Train_losses: [0.24837185442447662, 0.23836220800876617, 0.23395410180091858, 0.2309217005968094, 0.22854508459568024, 0.22665882110595703, 0.22489234805107117, 0.22335195541381836, 0.22191108763217926, 0.2163100242614746, 0.21173980832099915, 0.20977532863616943, 0.2082945704460144, 0.20679637789726257, 0.2034735232591629, 0.201302170753479, 0.20006850361824036, 0.19934922456741333, 0.19912199676036835, 0.1988079845905304, 0.19889014959335327]
0
24000
Validation_losses: [0.24508623778820038, 0.23523275554180145, 0.2308899313211441, 0.24815605580806732, 0.2290923297405243, 0.2281692773103714, 0.23243391513824463, 0.2334224283695221, 0.23414932191371918, 0.24953418970108032, 0.22492143511772156, 0.22605453431606293, 0.2261160910129547, 0.22700588405132294, 0.2279358208179474, 0.2282657027244568, 0.229500412940979, 0.22972556948661804, 0.22984717786312103, 0.22992195188999176, 0.22944648563861847]
decay loss from 9.765625e-07 to 4.8828125e-07 as not seeing improvement in val loss
created new optimizer with LR 4.8828125e-07
no improvement in 10 epochs, break
Training complete in 2877m 50s
saving
10
batch_size 48

